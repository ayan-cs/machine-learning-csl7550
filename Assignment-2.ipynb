{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "ML_Assignment-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Insurance Dataset**\r\n",
        "\r\n",
        "In the below implementations, I have used the [`insurance.csv` dataset](https://www.kaggle.com/awaiskaggler/insurance-csv). This dataset gives an idea about the insurance coverage of a person based on certain characteristics of that person. These characterics have been given as **features** and there are total ${1338}$ instances.\r\n",
        "\r\n",
        "- `Age` : $\\in \\mathbb{Z}$\r\n",
        "- `Sex` : $\\{Male=0, Female=1\\}$ \r\n",
        "- `BMI` : $\\in \\mathbb{R}$\r\n",
        "- `Children` : $\\in \\mathbb{Z}$\r\n",
        "- `Smoker` : $\\{No=0, Yes=1\\}$\r\n",
        "- `Region` : $\\{Northwest=0, Southwest=1\\}$\r\n",
        "\r\n",
        "The value that has to be predicted is the `Insurance Charges` which takes continuous values $(\\in \\mathbb{R})$"
      ],
      "metadata": {
        "id": "5LzLaw2VQiLH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\r\n",
        "df = pd.read_csv('./datasets/insurance.csv')\r\n",
        "df.head(5)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression using Gradient descent**\n",
        "\n",
        "As the dataset is having more than one feature, I have used **Batch Gradient Descent** algorithm for updating the weight vector represented by $\\underline w$.\n",
        "\n",
        "I have implemented each steps seperately and then I have used the inbuilt **`LinearRegression()`** from **`sklearn`** to compare the scores."
      ],
      "metadata": {
        "id": "DvwXDWesADLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Task\n",
        "\n",
        "There are some features which are categorical and take binary values. So the values have been converted into numerical values i.e. 0 and 1."
      ],
      "metadata": {
        "id": "1bMpnCkdlBkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "df = pd.read_csv('./datasets/insurance.csv')\r\n",
        "print(df.head(5))\r\n",
        "\r\n",
        "le = LabelEncoder()\r\n",
        "\r\n",
        "le.fit(df['sex'])\r\n",
        "df['sex'] = le.transform(df['sex'])\r\n",
        "\r\n",
        "le.fit(df['smoker'])\r\n",
        "df['smoker'] = le.transform(df['smoker'])\r\n",
        "\r\n",
        "le.fit(df['region'])\r\n",
        "df['region'] = le.transform(df['region'])\r\n",
        "\r\n",
        "print(df.head(5))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age     sex   bmi  children smoker     region  expenses\n",
            "0   19  female  27.9         0    yes  southwest  16884.92\n",
            "1   18    male  33.8         1     no  southeast   1725.55\n",
            "2   28    male  33.0         3     no  southeast   4449.46\n",
            "3   33    male  22.7         0     no  northwest  21984.47\n",
            "4   32    male  28.9         0     no  northwest   3866.86\n",
            "   age  sex   bmi  children  smoker  region  expenses\n",
            "0   19    0  27.9         0       1       3  16884.92\n",
            "1   18    1  33.8         1       0       2   1725.55\n",
            "2   28    1  33.0         3       0       2   4449.46\n",
            "3   33    1  22.7         0       0       1  21984.47\n",
            "4   32    1  28.9         0       0       1   3866.86\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdtqi6X7tkdP",
        "outputId": "89412512-9c26-4a4f-c925-0a4e086d0518"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Train and Test data\n",
        "\n",
        "The dataset is split into Train and Test set in ratio $70:30$ i.e. ${70\\%}$ in Train set and ${30\\%}$ in Test set."
      ],
      "metadata": {
        "id": "Mw0Kf19S7L8y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "X = np.array([df.loc[i][:-1] for i in range(len(df))])\r\n",
        "Y = np.array(df['expenses'])\r\n",
        "\r\n",
        "size = len(df)\r\n",
        "random_indice = np.random.permutation(size)\r\n",
        "num_train = int(size*0.7)\r\n",
        "num_test = int(size*0.3)\r\n",
        "\r\n",
        "X_train = X[random_indice[:num_train]]\r\n",
        "y_train = Y[random_indice[:num_train]]\r\n",
        "X_test = X[random_indice[-num_test:]]\r\n",
        "y_test = Y[random_indice[-num_test:]]"
      ],
      "outputs": [],
      "metadata": {
        "id": "JfoCfGr6hR1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of steps\n",
        "\n",
        "$size$ represents the total number of training instances, $\\alpha = 0.0001$ and number of iterations = $10^5$\n",
        "\n",
        "1. Start with some $\\underline w$\n",
        "2. Repeat until convergence (very large number of iterations)\n",
        "    1.  $\\underline h = <\\underline w, \\underline x>$\n",
        "    2.  $cost = \\frac 12 \\times size \\times \\sum{(\\underline h - \\underline y)^2}$\n",
        "    3.  $\\underline d_w = \\frac {d(cost)}{dw} = \\frac{1}{size} \\times (<(\\underline h - \\underline y), \\underline x>)$\n",
        "    4.  $\\underline w = \\underline w - \\alpha \\times \\underline d_w$\n",
        "\n",
        "Because I am calculating the inner product, so it is same as taking each training instances, multiplying with weight coefficients and taking the sum. Therefore, we get the derivatives in $\\underline d_w$ as a vector and hence the updated weights can easily be obtained in Step $2.4$"
      ],
      "metadata": {
        "id": "76cUoFq70c6U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import random\r\n",
        "w = [random.random() for _ in range(len(X_train[0]))]\r\n",
        "print(\"Initial weight : \",w)\r\n",
        "\r\n",
        "alpha = 0.0001\r\n",
        "epoch = 100000\r\n",
        "\r\n",
        "cost = []\r\n",
        "for i in range(epoch):\r\n",
        "  h = np.inner(w, X_train) # w=(1,6)  X_train=(n, 6)  h=(n,1)\r\n",
        "  cost.append(0.5*num_train*(np.sum(np.square(h - y_train))))\r\n",
        "  d_w = (1/num_train)*(np.inner(h - y_train, np.transpose(X_train))) #d_w=(1,6)\r\n",
        "  w = w - alpha*np.transpose(d_w)\r\n",
        "\r\n",
        "print(\"Final weight : \",w)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weight :  [0.8722695306496072, 0.7234165689405115, 0.10330294880805713, 0.09944336245092544, 0.983344204336581, 0.7163423787420305]\n",
            "Final weight :  [  214.35834607  -435.83863553    71.5076554    334.09112023\n",
            " 18526.56149504  -683.48450528]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV8eGjW5UGUU",
        "outputId": "8551a50b-87ea-40e4-f759-ea7173cb23ca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction and calculating $R^2$ score\n",
        "\n",
        "Prediction has been done by applying updated weights on test set. The observed $R^2$ score is = $0.6968$"
      ],
      "metadata": {
        "id": "8qcCoi3U5yRO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y_pred = np.inner(w, X_test)\r\n",
        "u = sum((y_pred - y_test)**2)\r\n",
        "v = sum((y_test - np.mean(y_test))**2)\r\n",
        "r2 = 1 - (u/v)\r\n",
        "print(\"Score : \"+str(r2))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score : 0.6968352930500166\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ON7FyZYxNMe",
        "outputId": "59374bb1-f35f-4850-e86a-50917a86d882"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Epoch-Cost plot\n",
        "\n",
        "From the plot below, the cost can be easily seen reducing at the end."
      ],
      "metadata": {
        "id": "4L5Zgmxp6QJa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.plot(np.arange(epoch), cost)\r\n",
        "plt.xlabel(\"Epochs\")\r\n",
        "plt.ylabel(\"Cost\")\r\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "yr0SECnoteKR",
        "outputId": "68f07c46-752e-44fd-a400-c57d4dfd58db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison with Scikits-learn's `LinearRegression()`\n",
        "\n",
        "Using the inbuilt `LinearRegression()` in `Scikits-learn`, the obtained $R^2$ score on the same test set is $0.7480$, which is close to the $R^2$ score obtained above."
      ],
      "metadata": {
        "id": "Joi1qGH96gEF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.linear_model import LinearRegression\r\n",
        "clf = LinearRegression()\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "print(\"Score : \"+str(clf.score(X_test, y_test)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score : 0.748009421569502\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l_azhPUzL2s",
        "outputId": "416ee970-16e5-4cb0-f21b-f44db369cbb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression using Pseudo-inverse method**\n",
        "\n",
        "It is a very straight-forward method to calculate the weight from given Train set. The general method is,\n",
        "\n",
        "$\\underline x \\times \\underline w=\\underline y$\n",
        "\n",
        "therefore, the unknown weights can be calculated as,\n",
        "\n",
        "$\\underline w = \\underline x^{-1} \\times \\underline y$\n",
        "\n",
        "If $\\underline x$ is not invertible, then the pseudo-inverse is calculated.\n"
      ],
      "metadata": {
        "id": "zlvql9_i9WEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Task\n",
        "\n",
        "There are some features which are categorical and take binary values. So the values have been converted into numerical values i.e. 0 and 1."
      ],
      "metadata": {
        "id": "NlTSGgmzwmX_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "df = pd.read_csv('./datasets/insurance.csv')\r\n",
        "print(df.head(5))\r\n",
        "\r\n",
        "le = LabelEncoder()\r\n",
        "\r\n",
        "le.fit(df['sex'])\r\n",
        "df['sex'] = le.transform(df['sex'])\r\n",
        "\r\n",
        "le.fit(df['smoker'])\r\n",
        "df['smoker'] = le.transform(df['smoker'])\r\n",
        "\r\n",
        "le.fit(df['region'])\r\n",
        "df['region'] = le.transform(df['region'])\r\n",
        "\r\n",
        "print(df.head(5))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age     sex   bmi  children smoker     region  expenses\n",
            "0   19  female  27.9         0    yes  southwest  16884.92\n",
            "1   18    male  33.8         1     no  southeast   1725.55\n",
            "2   28    male  33.0         3     no  southeast   4449.46\n",
            "3   33    male  22.7         0     no  northwest  21984.47\n",
            "4   32    male  28.9         0     no  northwest   3866.86\n",
            "   age  sex   bmi  children  smoker  region  expenses\n",
            "0   19    0  27.9         0       1       3  16884.92\n",
            "1   18    1  33.8         1       0       2   1725.55\n",
            "2   28    1  33.0         3       0       2   4449.46\n",
            "3   33    1  22.7         0       0       1  21984.47\n",
            "4   32    1  28.9         0       0       1   3866.86\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH7YXn5Twirp",
        "outputId": "dffe03e6-0757-433f-ae72-0faf2aafd551"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Train and Test data\n",
        "\n",
        "The dataset is split into Train and Test set in ratio $70:30$ i.e. ${70\\%}$ in Train set and ${30\\%}$ in Test set.\n",
        "\n"
      ],
      "metadata": {
        "id": "BsyZbcQc9vCr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "X = np.array([df.loc[i][:-1] for i in range(len(df))])\r\n",
        "Y = np.array(df['expenses'])\r\n",
        "\r\n",
        "size = len(df)\r\n",
        "random_indice = np.random.permutation(size)\r\n",
        "num_train = int(size*0.7)\r\n",
        "num_test = int(size*0.3)\r\n",
        "\r\n",
        "X_train = X[random_indice[:num_train]]\r\n",
        "y_train = Y[random_indice[:num_train]]\r\n",
        "X_test = X[random_indice[-num_test:]]\r\n",
        "y_test = Y[random_indice[-num_test:]]"
      ],
      "outputs": [],
      "metadata": {
        "id": "5OZtXvpQ9po9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculation of $\\underline w = \\underline x^{-1} \\times \\underline y$"
      ],
      "metadata": {
        "id": "wF-65xncxViH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if len(X_train)!=len(X_train[0]) or np.linalg.det(X_train)==0:\r\n",
        "  w1 = np.matmul(np.linalg.pinv(X_train), np.transpose(y_train))\r\n",
        "else :\r\n",
        "  w1 = np.matmul(np.transpose(y_train), np.linalg.inv(X_train))"
      ],
      "outputs": [],
      "metadata": {
        "id": "AaOsv_u_7Htu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction and calculating $R^2$ score\n",
        "\n",
        "Prediction has been done by applying updated weights on test set. The observed $R^2$ score is $\\approx 0.73$"
      ],
      "metadata": {
        "id": "wDvnkR2exhLJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y_pred = np.inner(w1, X_test)\r\n",
        "u = sum((y_pred - y_test)**2)\r\n",
        "v = sum((y_test - np.mean(y_test))**2)\r\n",
        "r2 = 1 - (u/v)\r\n",
        "print(\"Score : \"+str(r2))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score : 0.7299369857918232\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPSfJo34UMf2",
        "outputId": "099f572f-453c-4803-9134-fbdb4fef9336"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison with Scikits-learn's `LinearRegression()`\n",
        "\n",
        "Using the inbuilt `LinearRegression()` in `Scikits-learn`, the obtained $R^2$ score on the same test set is $\\approx 0.76$, which is close to the $R^2$ score obtained above."
      ],
      "metadata": {
        "id": "pfRF4hnAxr9M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.linear_model import LinearRegression\r\n",
        "clf = LinearRegression()\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "print(\"Score : \"+str(clf.score(X_test, y_test)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score : 0.7615234860015696\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqj8HU6vLmq1",
        "outputId": "dfcb566c-989d-4b28-bf93-b226b970f79c"
      }
    }
  ]
}